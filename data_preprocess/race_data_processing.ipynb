{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_high = 'train/high/'\n",
    "train_dir_middle = 'train/middle/'\n",
    "\n",
    "dev_dir_high = 'dev/high/'\n",
    "dev_dir_middle = 'dev/middle/'\n",
    "\n",
    "test_dir_high = 'test/high/'\n",
    "test_dir_middle = 'test/middle/'\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def read_race(file_dir, output_file):\n",
    "    output = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            with open(os.path.join(file_dir, file), 'r') as f:\n",
    "                json_str = f.read()\n",
    "                output.append(json.loads(json_str))\n",
    "#     print(output[0])\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output, f)\n",
    "        \n",
    "bert_vocab = '../../CoQA-Challenge/BERT/bert-base-uncased-vocab.txt'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all single text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_race(dev_dir_high, 'dev-high.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_race(dev_dir_middle, 'dev-middle.json')\n",
    "read_race(test_dir_middle, 'test-middle.json')\n",
    "read_race(test_dir_high, 'test-high.json')\n",
    "read_race(train_dir_middle, 'train-middle.json')\n",
    "read_race(train_dir_high, 'train-high.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine dataset for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write train-middle.json and train-high.json into train-combine.json, in total 25137\n",
      "Write dev-middle.json and dev-high.json into dev-combine.json, in total 1389\n",
      "Write test-middle.json and test-high.json into test-combine.json, in total 1407\n"
     ]
    }
   ],
   "source": [
    "def combine(middle_file, high_file, output_file):\n",
    "    middle = json.load(open(middle_file, 'r'))\n",
    "    high = json.load(open(high_file, 'r'))\n",
    "    for instance in middle:\n",
    "        instance['id'] = 'm-' + instance['id']\n",
    "    for instance in high:\n",
    "        instance['id'] = 'h-' + instance['id']\n",
    "    combined = middle + high\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(combined, f)\n",
    "    print(f'Write {middle_file} and {high_file} into {output_file}, in total {len(combined)}')\n",
    "    \n",
    "combine('train-middle.json', 'train-high.json', 'train-combine.json')\n",
    "combine('dev-middle.json', 'dev-high.json', 'dev-combine.json')\n",
    "combine('test-middle.json', 'test-high.json', 'test-combine.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def read_race_articles(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    articles = []\n",
    "    for instance in tqdm(data):\n",
    "        articles.append({\n",
    "            'id': instance['id'],\n",
    "            'article': instance['article']\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "def read_coqa_articles(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)['data']\n",
    "    articles = []\n",
    "    for instance in tqdm(data):\n",
    "        articles.append({\n",
    "            'id': instance['id'],\n",
    "            'article': instance['story']\n",
    "        })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1021/1021 [00:00<00:00, 609418.58it/s]\n",
      "100%|██████████| 368/368 [00:00<00:00, 550072.66it/s]\n",
      "100%|██████████| 6409/6409 [00:00<00:00, 965078.42it/s]\n",
      "100%|██████████| 18728/18728 [00:00<00:00, 594362.33it/s]\n",
      "100%|██████████| 7199/7199 [00:00<00:00, 845745.18it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 336459.49it/s]\n"
     ]
    }
   ],
   "source": [
    "race_train_articles = []\n",
    "race_dev_articles = []\n",
    "for file in ['dev-high.json', 'dev-middle.json', 'train-middle.json', 'train-high.json']:\n",
    "    data = read_race_articles(file)\n",
    "    if 'train' in file:\n",
    "        race_train_articles.extend(data)\n",
    "    else:\n",
    "        race_dev_articles.extend(data)\n",
    "        \n",
    "coqa_train = '/home/jiaofangkai/CoQA-Challenge/BERT_RC/data-set/coqa-train-v1.0.json'\n",
    "coqa_dev = '/home/jiaofangkai/CoQA-Challenge/BERT_RC/data-set/coqa-dev-v1.0.json'\n",
    "\n",
    "coqa_train_articles = read_coqa_articles(coqa_train)\n",
    "coqa_dev_articles = read_coqa_articles(coqa_dev)\n",
    "\n",
    "with open('race-coqa-train-articles.json', 'w') as f:\n",
    "    json.dump(coqa_train_articles + race_train_articles, f)\n",
    "with open('race-coqa-dev-articles.json', 'w') as f:\n",
    "    json.dump(coqa_dev_articles + race_dev_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7199/7199 [00:00<00:00, 881831.56it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 407372.18it/s]\n"
     ]
    }
   ],
   "source": [
    "coqa_train = '/home/jiaofangkai/CoQA-Challenge/BERT_RC/data-set/coqa-train-v1.0.json'\n",
    "coqa_dev = '/home/jiaofangkai/CoQA-Challenge/BERT_RC/data-set/coqa-dev-v1.0.json'\n",
    "\n",
    "coqa_train_articles = read_coqa_articles(coqa_train)\n",
    "coqa_dev_articles = read_coqa_articles(coqa_dev)\n",
    "\n",
    "with open('coqa-dev-articles.json', 'w') as f:\n",
    "    json.dump(coqa_dev_articles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25137/25137 [00:16<00:00, 1495.66it/s]\n"
     ]
    }
   ],
   "source": [
    "len_cnt = Counter()\n",
    "\n",
    "for instance in tqdm(race_train_articles):\n",
    "    len_cnt[len(sentence_tokenizer.tokenize(instance['article']))] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "a_cnt = 0\n",
    "for key in len_cnt:\n",
    "    if key > 90:\n",
    "        a_cnt += 1\n",
    "print(a_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1021/1021 [00:00<00:00, 413317.67it/s]\n",
      "100%|██████████| 368/368 [00:00<00:00, 356549.75it/s]\n",
      "100%|██████████| 6409/6409 [00:00<00:00, 565588.59it/s]\n",
      "100%|██████████| 18728/18728 [00:00<00:00, 154299.46it/s]\n"
     ]
    }
   ],
   "source": [
    "race_train_articles = []\n",
    "race_dev_articles = []\n",
    "for file in ['dev-high.json', 'dev-middle.json', 'train-middle.json', 'train-high.json']:\n",
    "    data = read_race_articles(file)\n",
    "    if 'train' in file:\n",
    "        race_train_articles.extend(data)\n",
    "    else:\n",
    "        race_dev_articles.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25137/25137 [02:55<00:00, 143.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21845\n",
      "{\n",
      "  \"20\": 1404,\n",
      "  \"23\": 847,\n",
      "  \"13\": 1341,\n",
      "  \"11\": 1050,\n",
      "  \"12\": 1189,\n",
      "  \"14\": 1527,\n",
      "  \"16\": 1746,\n",
      "  \"27\": 405,\n",
      "  \"17\": 1693,\n",
      "  \"24\": 752,\n",
      "  \"15\": 1635,\n",
      "  \"21\": 1152,\n",
      "  \"29\": 289,\n",
      "  \"26\": 497,\n",
      "  \"22\": 999,\n",
      "  \"36\": 64,\n",
      "  \"25\": 668,\n",
      "  \"19\": 1502,\n",
      "  \"18\": 1656,\n",
      "  \"28\": 369,\n",
      "  \"39\": 34,\n",
      "  \"30\": 227,\n",
      "  \"37\": 47,\n",
      "  \"31\": 179,\n",
      "  \"33\": 115,\n",
      "  \"38\": 38,\n",
      "  \"32\": 117,\n",
      "  \"40\": 34,\n",
      "  \"43\": 13,\n",
      "  \"35\": 73,\n",
      "  \"34\": 101,\n",
      "  \"41\": 21,\n",
      "  \"45\": 4,\n",
      "  \"44\": 13,\n",
      "  \"46\": 7,\n",
      "  \"49\": 4,\n",
      "  \"48\": 4,\n",
      "  \"42\": 14,\n",
      "  \"47\": 6,\n",
      "  \"57\": 1,\n",
      "  \"51\": 1,\n",
      "  \"50\": 3,\n",
      "  \"52\": 2,\n",
      "  \"53\": 1,\n",
      "  \"58\": 1\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 500\n",
    "sen_cnt = Counter()\n",
    "\n",
    "for instance in tqdm(race_train_articles):\n",
    "    article = instance['article']\n",
    "    \n",
    "    sentences = sentence_tokenizer.tokenize(article)\n",
    "    \n",
    "    total_piece = 0\n",
    "    sentence_num = 0\n",
    "    for sentence in sentences:\n",
    "        piece_num = len(bert_tokenizer.tokenize(sentence))\n",
    "        if total_piece + piece_num > max_seq_length:\n",
    "            total_piece = piece_num\n",
    "            if sentence_num <= 10:\n",
    "                sentence_num = 1\n",
    "                continue\n",
    "            sen_cnt[sentence_num] += 1\n",
    "            sentence_num = 1\n",
    "        else:\n",
    "            total_piece += piece_num\n",
    "            sentence_num += 1\n",
    "        \n",
    "    if sentence_num > 10:\n",
    "        sen_cnt[sentence_num] += 1\n",
    "        \n",
    "print(sum(sen_cnt.values()))\n",
    "print(json.dumps(sen_cnt, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25137/25137 [02:46<00:00, 151.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21889\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 400\n",
    "\n",
    "train_segments = []\n",
    "\n",
    "for instance in tqdm(race_train_articles):\n",
    "    article = instance['article']\n",
    "    \n",
    "    sentences = sentence_tokenizer.tokenize(article)\n",
    "    \n",
    "    total_piece = 0\n",
    "    segment = []\n",
    "    for sentence in sentences:\n",
    "        piece_num = len(bert_tokenizer.tokenize(sentence))\n",
    "        if total_piece + piece_num > max_seq_length:\n",
    "            total_piece = piece_num\n",
    "            if len(segment) > 10:\n",
    "                train_segments.append(\" \".join(segment))\n",
    "            segment = [sentence]\n",
    "        else:\n",
    "            total_piece += piece_num\n",
    "            segment.append(sentence)\n",
    "        \n",
    "    if len(segment) > 10:\n",
    "        train_segments.append(\" \".join(segment))\n",
    "\n",
    "with open('train-segments.json', 'w') as f:\n",
    "    json.dump(train_segments, f)\n",
    "print(len(train_segments))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1389/1389 [00:10<00:00, 132.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_segments = []\n",
    "for instance in tqdm(race_dev_articles):\n",
    "    article = instance['article']\n",
    "    sentences = sentence_tokenizer.tokenize(article)\n",
    "    \n",
    "    total_piece = 0\n",
    "    segment = []\n",
    "    for sentence in sentences:\n",
    "        piece_num = len(bert_tokenizer.tokenize(sentence))\n",
    "        if total_piece + piece_num > max_seq_length:\n",
    "            total_piece = piece_num\n",
    "            if len(segment) > 10:\n",
    "                dev_segments.append(\" \".join(segment))\n",
    "            segment = [sentence]\n",
    "        else:\n",
    "            segment.append(sentence)\n",
    "            total_piece += piece_num\n",
    "    \n",
    "    if len(segment) > 10:\n",
    "        dev_segments.append(\" \".join(segment))\n",
    "with open('dev-segments.json', 'w') as f:\n",
    "    json.dump(dev_segments, f)\n",
    "print(len(dev_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
